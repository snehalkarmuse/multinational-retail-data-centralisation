{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527c2124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql+psycopg2://postgres:***@localhost:5432/postgres)\n",
      "Engine(postgresql+psycopg2://postgres:***@localhost:5432/postgres)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120123 entries, 0 to 120122\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   level_0           120123 non-null  int64 \n",
      " 1   index             120123 non-null  int64 \n",
      " 2   date_uuid         120123 non-null  object\n",
      " 3   user_uuid         120123 non-null  object\n",
      " 4   card_number       120123 non-null  int64 \n",
      " 5   store_code        120123 non-null  object\n",
      " 6   product_code      120123 non-null  object\n",
      " 7   product_quantity  120123 non-null  int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 7.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import db_connect as dbc\n",
    "import data_extraction as dbe\n",
    "import datetime as dt\n",
    "from sqlalchemy import text\n",
    "import numpy as np\n",
    "import re\n",
    "class DataCleaning:\n",
    "    def clean_user_data(self):\n",
    "        \n",
    "        self.df = db_extractor.read_rds_table(db_connector,\"legacy_users\")\n",
    "        \n",
    "        # check any null value\n",
    "        print(\"Any Null values:\", self.df.isnull().values.any())\n",
    "        # drop null \n",
    "        self.df = self.df.dropna()\n",
    "        # null count in whole table\n",
    "        na_count = self.df.isna().sum().sum()\n",
    "       \n",
    "        # any duplicates in table. count rows\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        no_duplicate_row_count = self.df.shape[0]\n",
    "        print(\"No Duplicate:\", no_duplicate_row_count)    \n",
    "\n",
    "        self.df.drop(self.df.loc[self.df['first_name'].str.contains(r'[0-9]') == True].index,axis = 0,inplace = True)\n",
    "        self.df.drop(self.df.loc[self.df['last_name'].str.contains(r'[0-9]') == True].index,axis = 0,inplace = True)\n",
    "        \n",
    "        self.df.drop(self.df.loc[self.df['email_address'].str.contains('@') == False].index,axis = 0,inplace = True)\n",
    "        \n",
    "        self.df.drop(self.df.loc[self.df['country'].str.contains(r'[0-9]') == True].index,axis = 0,inplace = True)\n",
    "        \n",
    "        self.df.drop(self.df.loc[self.df['country_code'].str.len() != 2].index,axis = 0,inplace = True)\n",
    "       \n",
    "        self.df.drop(self.df.loc[self.df['phone_number'].str.contains(r'[a-z]') == True].index,axis = 0,inplace = True)\n",
    "        self.df['phone_number']= self.df['phone_number'].replace(['(',')',\"+\",\".\",\"-\"],\" \",inplace = True)\n",
    "        #self.df['phone_number'] = self.df['phone_number'].str.strip()\n",
    "        print(self.df.shape[0])\n",
    "        #self.df = self.df[['date_of_birth','join_date']].dt.strftime('%y-%m-%d')\n",
    "        self.df.drop(self.df.loc[self.df['date_of_birth'].str.contains('NULL', case = False)].index, axis = 0,inplace=True)\n",
    "        self.df.drop(self.df.loc[self.df['date_of_birth'].str.contains(r'[a-z]', case = False)].index,axis = 0,inplace=True)\n",
    "        self.df.drop(self.df.loc[self.df['date_of_birth'].str.contains('-') == False].index,axis = 0,inplace = True)\n",
    "        self.df['date_of_birth'] = self.df['date_of_birth'].astype('datetime64[ns]')\n",
    "        self.df['date_of_birth'] = pd.to_datetime(self.df[\"date_of_birth\"]).dt.normalize()\n",
    "        self.df['date_of_birth'] = pd.to_datetime(self.df['date_of_birth']).dt.date\n",
    "        self.df.drop(self.df.loc[self.df['join_date'].str.contains('-') == False].index,axis = 0,inplace = True)\n",
    "        self.df['join_date'] = self.df['join_date'].astype('datetime64[ns]')\n",
    "        self.df['join_date'] = pd.to_datetime(self.df[\"join_date\"]).dt.normalize()\n",
    "        self.df['join_date'] = pd.to_datetime(self.df['join_date']).dt.date\n",
    "        \n",
    "    def clean_card_data(self, dataframe):\n",
    "        list_of_df = []\n",
    "        for card_df in dataframe:\n",
    "            local_card_dataframe = card_df\n",
    "            local_card_dataframe = local_card_dataframe.dropna()\n",
    "            local_card_dataframe.drop_duplicates()\n",
    "            #change the datatype from int to string and then delete records whose card number is not equal to 16.\n",
    "            local_card_dataframe['card_number'] = local_card_dataframe['card_number'].apply(str)\n",
    "            local_card_dataframe.drop(local_card_dataframe.loc[local_card_dataframe['card_number'].str.len() != 16].index, axis = 0, inplace= True)\n",
    "            # converting date_payment_confirmed first into date datatype andformating it. infer_datetime parameter will delete previous formating\n",
    "            local_card_dataframe['date_payment_confirmed'] = pd.to_datetime(local_card_dataframe['date_payment_confirmed'],infer_datetime_format=True, format='%Y%m%d')\n",
    "            local_card_dataframe['date_payment_confirmed'] = local_card_dataframe['date_payment_confirmed'].apply(str)\n",
    "            list_of_df.append(local_card_dataframe)\n",
    "            return list_of_df\n",
    "\n",
    "    def called_clean_store_data(self, store_dataframe):\n",
    "        self.store_dataframe = store_dataframe\n",
    "        self.store_dataframe.pop('lat')\n",
    "        self.store_dataframe = self.store_dataframe.dropna()\n",
    "        self.store_dataframe.drop_duplicates()   \n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['longitude'].str.contains(r'[0-9]') != True].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['staff_numbers'].str.contains(r'[0-9]') != True].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['latitude'].str.contains(r'[0-9]') != True].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe['continent'] = self.store_dataframe['continent'].str.replace('eeEurope','Europe')\n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['continent'].str.contains(r'[a-z]') == False].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['locality'].str.contains(r'[a-z]') == False].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe.drop(self.store_dataframe.loc[self.store_dataframe['store_type'].str.contains(r'[a-z]') == False].index,axis = 0,inplace = True)\n",
    "        self.store_dataframe['opening_date']= pd.to_datetime(self.store_dataframe['opening_date'], infer_datetime_format=True)\n",
    "        return self.store_dataframe\n",
    "\n",
    "    def clean_product_data(self,product_df):\n",
    "        self.product_df = product_df\n",
    "        self.product_df.drop_duplicates()\n",
    "        self.product_df['date_added']= pd.to_datetime(self.product_df['date_added'], infer_datetime_format=True, errors='coerce')\n",
    "        self.product_df = self.product_df.dropna()\n",
    "        return self.product_df\n",
    "    \n",
    "    def convert_product_weights(self,product_df):\n",
    "        s3_data.drop(s3_data.loc[s3_data['weight'].str.match(r'(\\d*\\.?\\d+)(kg|g|l|ml)') == False].index,axis = 0,inplace = True)\n",
    "        \n",
    "        s3_data['weight'] = s3_data['weight'].apply(lambda x: self.weight_conversion(x))\n",
    "       \n",
    "            \n",
    "    def weight_conversion(self,weight_str):\n",
    "        weight_rgx = re.compile(r'(\\d*\\.?\\d+)(kg|g|l|ml)')\n",
    "        mo = weight_rgx.search(weight_str)\n",
    "        wn = float(mo.group(1)) \n",
    "        if mo.group(2)=='g' or mo.group(2)=='ml':\n",
    "            wn = wn/1000\n",
    "        return wn\n",
    "\n",
    "    def clean_order_data(self):\n",
    "        self.order_df = db_extractor.read_rds_table(db_connector,\"orders_table\")\n",
    "        \n",
    "        new_order_df = self.order_df.drop(['first_name','last_name','1'], axis = 1)\n",
    "        print( new_order_df.info())\n",
    "\n",
    "data_retrieve_link = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/\"\n",
    "store_api_config = {\"x-api-key\":\"yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX\"}\n",
    "db_connector = dbc.DatabaseConnector()\n",
    "db_connector.init_db_engine()\n",
    "db_extractor = dbe.DataExtractor()\n",
    "db_cleaning = DataCleaning()\n",
    "#db_cleaning.clean_user_data()\n",
    "'''db_connector.upload_to_db(db_cleaning.df,'dim_user')\n",
    "list_of_df = db_cleaning.clean_card_data(db_extractor.retrieve_pdf_data(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"))\n",
    "for i in list_of_df:\n",
    "    db_connector.upload_to_db(i,'dim_card_detail')\n",
    "no_of_stores = db_extractor.list_number_of_stores('https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores',store_api_config)\n",
    "store_dataframe= db_extractor.add_data_to_dataframe(data_retrieve_link, no_of_stores,store_api_config)\n",
    "clean_store_dataframe = db_cleaning.called_clean_store_data(store_dataframe)\n",
    "db_connector.upload_to_db(db_cleaning.store_dataframe,'dim_store_details')'''\n",
    "\n",
    "s3_data = db_extractor.extract_from_s3(\"s3://data-handling-public/products.csv\")\n",
    "s3_data = db_cleaning.clean_product_data(s3_data)\n",
    "db_cleaning.convert_product_weights(s3_data)\n",
    "db_connector.upload_to_db(db_cleaning.product_df,'dim_products')\n",
    "db_cleaning.clean_order_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931c747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
